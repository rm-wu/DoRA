#!/bin/bash -l

#SBATCH --job-name=dora_pretrain   # Job name
#SBATCH --output=/projappl/project_2006256/mereuric/DoRA/logs/dora.o%j # Name of stdout output file
#SBATCH --error=/projappl/project_2006256/mereuric/DoRA/logs/dora.e%j  # Name of stderr error file
#SBATCH --partition=gpumedium  # partition name
#SBATCH --nodes=2               # Total number of nodes 
#SBATCH --ntasks-per-node=4     # 8 MPI ranks per node, 16 total (2x8)
#SBATCH --gres=gpu:a100:4,nvme:10       # Allocate one gpu per MPI rank
#SBATCH --time=36:00:00       # Run time (d-hh:mm:ss)
#SBATCH --account=project_2006256  # Project for billing
#SBATCH --cpus-per-task=32
#SBATCH --mem=200G
#SBATCH --mail-type=ALL
#SBATCH --mail-user=riccardo.mereu@aalto.fi
#SBATCH --hint=nomultithread

module purge
module load tykky

export PATH="/projappl/project_2006256/mereuric/DoRA/venv/bin:$PATH"

mkdir -p $LOCAL_SCRATCH/WTDataset/
cp /scratch/project_2006256/data/WTDataset/venice.mp4 $LOCAL_SCRATCH/WTDataset/

set -x

python -m torch.distributed.launch --nproc_per_node=4 main.py --arch vit_small \
    --data_path $LOCAL_SCRATCH/WTDataset/venice.mp4 \
    --output_dir /projappl/project_2006256/mereuric/DoRA/logs/weights_WTours/DoRA/venice_8frames/ \
    --optimizer adamw \
    --use_bn_in_head False \
    --out_dim 65536 \
    --batch_size_per_gpu 16 \
    --local_crops_number 6 \
    --epochs 100 \
    --num_workers 32 \
    --lr 0.0005 \
    --min_lr 0.00001 \
    --norm_last_layer False \
    --warmup_teacher_temp_epochs 30 \
    --weight_decay 0.04 \
    --weight_decay_end 0.4 \
    --frame_per_clip 8 \
    --step_between_clips 60

